{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Wrangle Report</h1> </center>\n",
    "\n",
    "***\n",
    "\n",
    "<br />\n",
    "\n",
    "## Introduction :\n",
    "In this project I've practiced what I learned in **wrangling data** section from Udacity Data Analysis Nanodegree program.\n",
    "The data that is wrangled here is the tweet archive of Twitter user [**@dog_rates**](https://twitter.com/dog_rates), also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog.\n",
    "\n",
    "This report briefly describes my data wrangling efforts.\n",
    "\n",
    "## Gathering Data\n",
    "\n",
    "In this section I've gathered 3 datasets using 3 different methods, from 3 different sources in 3 different formats, see the table below:\n",
    "\n",
    "| Num \t| Dataset           \t| Method            \t| Source                                                                                                                                             \t| Format     \t| Content                                                                    \t|\n",
    "|:-----\t|:-------------------\t|:-------------------\t|:----------------------------------------------------------------------------------------------------------------------------------------------------\t|:------------\t|:----------------------------------------------------------------------------\t|\n",
    "| 1   \t| Twiter_Archive    \t| Manually          \t| Local: given file [here](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv): \t| .csv file  \t| Original tweets having data about dogs like image, breed, rate and more.   \t|\n",
    "| 2   \t| Image_predictions \t| Programmatically  \t| Web: This [link](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)                        \t| .tsv file  \t| image predictions, i.e., what breed of dog (or other object, animal, etc.) \t|\n",
    "| 3   \t| Twitter_API       \t| Querying API      \t| Twitter [API](https://en.wikipedia.org/wiki/Twitter)                                                                                               \t| .JSON file \t| (retweet/like)count related to the original tweets.                        \t|\n",
    "\n",
    "<br />\n",
    "\n",
    "## Assessing Data\n",
    "Detecting visually and programmatically the quality and tidiness issues and documenting them.\n",
    "\n",
    "### Quality\n",
    "\n",
    "#### `twitter_archive` table \n",
    "1. `name` missing values '**None**' as strings instead of **`Null`**.\n",
    "2. `'None'` entered as `Null` value in `doggo`, `floofer`, `pupper` and `puppo` columns.\n",
    "3. `name` contains some wrong values e.g:(a, an, the).\n",
    "4. `tweet_id` datatype is int64 instead of string.\n",
    "5. `timestamp` datatype is object instead of datetime.\n",
    "6. some erroneous data in rating values.\n",
    "7. exaggerated `rating_numerator` high values i.e (value = 1776).\n",
    "8. there exist some retweets that shouled deleted.\n",
    "9. unneeded columns for analysis process.\n",
    "10. 14 tweets take two stages at once.\n",
    "\n",
    "\n",
    "#### `image_predictions` table\n",
    "\n",
    "11. `tweet_id` datatype is int64 instead of string.\n",
    "12. existence of 66 duplicated data in `jpg_url` column.\n",
    "13. some inconsisitent capital words and underscores in `p1`, `p2` and `p3` columns.\n",
    "\n",
    "#### `twitter_api` table\n",
    "14. `id` column name instead of `tweet_id`.\n",
    "15. `id` is int64 instead of string.\n",
    "\n",
    "### Tidiness\n",
    "1. in `twitter_archive` dataset, one variable in four columns i.e (doggo, floofer, pupper, puppo) instead of stage.\n",
    "2. `twitter_archive`, `image_predictions` and `twitter_api` datasets should be joined.\n",
    "3. in `twitter_archive` date and time in the same column `timestamp`.\n",
    "\n",
    "<br />\n",
    "\n",
    "## Cleaning Data\n",
    "Cleaning all the quality and tidiness issues documented in the assessing section using Python's libraries and their methods:\n",
    "\n",
    "1. Extract dog stages from `text` column to new column called `dog_stage` then convet them to lowercase for consistency purpose, and drop `doggo`, `floofer`, `pupper` and `puppo` columns.\n",
    "2. Convert `timestamp` from object to datetime type.\n",
    "3. Create two columns for `date` and `time` variables based on column `timestamp`column.\n",
    "4. Convert the type of `tweet_id` column from `int64` to `str`.\n",
    "5. Rename `id` column name to `tweet_id`.\n",
    "6. Convert the type of `tweet_id` column from `int64` to `str`.\n",
    "7. Drop duplicates data in `jpg_url` column.\n",
    "8. Merge `twitter_archive`, `image_predictions` and `twitter_api` datasets using pandas' `merge` function .\n",
    "9. Delete all rows that contain retweets.\n",
    "10. Delete the following columns: `source`, `in_reply_to_status_id`, `in_reply_to_user_id`,\n",
    "`retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp`, `expanded_urls` and `timestamp`.\n",
    "11. Make the inexistent names as `None` in `name` column.\n",
    "12. Replace 'None' by `Null` using `nan` NumPys' function.\n",
    "13. Remove underscores from `p1`, `p2`, `p3` columns and change the first letter to capital letter.\n",
    "14. Extracting rating values from `text` column to new column. \n",
    "15. Separate the numerator from the denominator in two new columns using `split` function.\n",
    "16. Correct the wrong numerator value from '1776' to '17.76' in `rating_numerator` column.\n",
    "17. Create new column called `rating` hold the result of dividing (numerator / denominator).\n",
    "18. Drop unneeded column.\n",
    "19. Sort the dataset columns in appropriate manner.\n",
    "20. Store the cleaned data in .csv file called `twitter_archive_master.csv`.\n",
    "\n",
    "## Conclusion\n",
    "After finished the Data Wrangling process (Gathering, Assessing, Cleaning and storing data), now our data ( `Twitter archive` joined with `image predictions` and `Twitter API`) is ready to be analyzed and visualized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
